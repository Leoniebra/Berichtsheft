\subsection{27. Juli}
Heute habe ich begonnen mich in das frisch aufgesetzte Loki-System einzuarbeiten, und die Abfragesprache LogQL (angelehnt an Prometheus' PromQL) auf die in das System eingespeisten Logs anzuwenden. Ein großer Unterschied zwischen Grafana Loki und Elastic Search besteht darin, dass Loki keine Datenattributierung beim Ingesten vornimmt, sondern die Logs als mit wenigen Indexen versehene strings ablegt. Die Erzeugung durchsuchbarer Indizes geschieht zur Durchsuchungszeit "per Hand" - zum Beispiel habe ich per RegEx mithilfe von named capturing groups das log level, den HTTP Status und die request time aus nginx logdaten auslesen können. Mein Mitarbeiter und ich haben im Anschluss an die kurze explorative Phase die Vor- und Nachteile von Elasticsearch und Grafana Loki evaluiert. Wir sind hierbei zu dem Schluss gekommen, dass beide Systeme einen wertvollen Mehrwert zum Monitoringprojekt liefern, und es für die Größe unserer Architektur sowohl realistisch als auch sinnvoll ist die Systeme parallel zu betreiben. Hierbei soll Loki als initiales Fehlererkundungstool für Enwickler:innen genutzt werden - hierfür qualifiziert es sich durch die direkte Bereitstellung der Log-Daten und die hochflexible Abfragesprache. Elasticsearch dagegen soll für die Generierung von tieferen Insights genutzt werden, sowie zur Erstellung von Logbasierten Alerts - hier spricht für Elastic dass es eine deutlich reichere Datenstruktur hat und es erlaubt eine gründliche Voraufbereitung von Daten voll auszunutzen. Zum Senden von Alerts durch Elastic Search mit der open Source Code-Base sind wir gezwungen die Alerts mittels Elasticsearch in einen eigens dafür reservierten Index zu schreiben. Dieser Index wird von Logstash dann gelesen und aufbereitet. Wir werden dann ein eigenes Stück software schreiben müssen, um diese Logs an den Alertmanager zu pushen. 

Zuletzt hat mein Mitarbeiter mir aufbauend auf meiner eigenständigen Einarbeitung in Docker und dem gestrigen Gespräch über Contaioner noch ein Beispiel dafür gezeigt wie wir im Alltagsbetrieb docker-compose nutzen, und wir haben die docker-compose.yaml des Monitoring-Servers (auf welchem alle Anwendungen in Docker-Containern laufen) durchgesprochen. Hier waren für mich besonders die Nutzung von Docker Netzwerken und Volumes interessant. WWir haben außerdem noch das Konzept von VM-Snapshots besprochen und auf Betriebssystemkernelebene erklärt. Das konzept war im Rahmen eines von meinem Vorgesetzten aufgebrachten Punktes zur Sprache gekommen - dieser hatte angestoßen, ob die Vorbereitung von Windows-Servern für automatische Software updates mithilfe einer Azure-DevOps-Pipeline automatisiert werden könnte. Dieser Anstoß hatte zu einem Gespräch über die Automatisierung von Datensicherung und -wiederherstellung mithilfe von Powershell und bash angestoßen.